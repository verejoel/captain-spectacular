{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tensorflow import set_random_seed\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.models import Sequential\n",
    "# sequential model, for linear stack of NN layers\n",
    "\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "# Dense is a standard densely connected NN layer\n",
    "# Dropout randomly drops a percentage, p, of the neurons from a layer (helps reduce overfitting)\n",
    "\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "# Convolution layers that help efficiently train image data\n",
    "\n",
    "from keras.utils import np_utils\n",
    "# used for transforming data\n",
    "\n",
    "# read training and testing data\n",
    "X_train = pd.read_pickle(\"../training_data/X_train.bz2\").values\n",
    "Y_train = pd.read_pickle(\"../training_data/Y_train.bz2\").values\n",
    "X_test = pd.read_pickle(\"../training_data/X_test.bz2\").values\n",
    "Y_test = pd.read_pickle(\"../training_data/Y_test.bz2\").values\n",
    "\n",
    "p = np.shape(X_train)[0] # number of training data\n",
    "q = np.shape(X_test)[0] # number of test data\n",
    "\n",
    "m = np.shape(X_train)[1]/2 # number of (T,cv) data\n",
    "n = np.shape(Y_train)[1] # dimension of output vector\n",
    "\n",
    "X_train = np.reshape(X_train, (p, 2, m, 1))\n",
    "X_test = np.reshape(X_test, (q, 2, m, 1))\n",
    "\n",
    "# check the loaded data\n",
    "r = np.random.randint(0,p,5)\n",
    "print r\n",
    "for i in r:\n",
    "    plt.figure(0)\n",
    "    plt.plot(X_train[i][0],X_train[i][1],'+')\n",
    "    plt.figure(1)\n",
    "    plt.plot(Y_train[i])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# preprocess input data\n",
    "\n",
    "# cv lies in the range [0,1], whereas T is in the range [0.1,300]\n",
    "# we should feature scale T by dividing by 300 (typical maximum value for real data)\n",
    "\n",
    "X_train[:,0,:,0] /= 300.\n",
    "X_test[:,0,:,0] /= 300.\n",
    "# normalize so values range between 0 and 1\n",
    "\n",
    "# check the processed data\n",
    "r = np.random.randint(0,p,5)\n",
    "print r\n",
    "for i in r:\n",
    "    plt.figure(0)\n",
    "    plt.plot(X_train[i][0],X_train[i][1],'+')\n",
    "    plt.figure(1)\n",
    "    plt.plot(Y_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------\n",
    "# Define custom loss functions for regression in Keras \n",
    "#-----------------------------------------------------------------------------\n",
    "#\n",
    "# REF: https://github.com/keras-team/keras/issues/7947\n",
    "\n",
    "from keras import backend\n",
    "\n",
    "# root mean squared error (rmse) for regression\n",
    "def rmse(y_true, y_pred):\n",
    "    return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "# mean squared error (mse) for regression\n",
    "def mse(y_true, y_pred):\n",
    "    return backend.mean(backend.square(y_pred - y_true), axis=-1)\n",
    "\n",
    "# coefficient of determination (R^2) for regression\n",
    "def r_square(y_true, y_pred):\n",
    "    SS_res =  backend.sum(backend.square(y_true - y_pred)) \n",
    "    SS_tot = backend.sum(backend.square(y_true - backend.mean(y_true))) \n",
    "    return (1 - SS_res/(SS_tot + backend.epsilon()))\n",
    "\n",
    "def r_square_loss(y_true, y_pred):\n",
    "    SS_res =  backend.sum(backend.square(y_true - y_pred)) \n",
    "    SS_tot = backend.sum(backend.square(y_true - backend.mean(y_true))) \n",
    "    return 1 - ( 1 - SS_res/(SS_tot + backend.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom loss function\n",
    "#\n",
    "# minimizes difference in output dos and in calculated heat capacity\n",
    "#\n",
    "# boltzmann constant in meV per Kelvin\n",
    "kB = 0.08617\n",
    "\n",
    "# phonon DOS properties (units are meV)\n",
    "n = 240 # length of output array\n",
    "wmax = 120 # ouput spectra will cover range [dw, wmax]\n",
    "dw = wmax/(1.*n) # spectrum bin width\n",
    "\n",
    "# temperature vector properties (units are Kelvin)\n",
    "m = 300 # length of temperature array\n",
    "Tmin = 1 # minimum temp\n",
    "Tmax = 300 # maximum temp\n",
    "\n",
    "def makeT(m):\n",
    "    return np.linspace(Tmin, Tmax, m)\n",
    "\n",
    "def makeW(n):\n",
    "    return np.linspace(dw, wmax, n)\n",
    "\n",
    "def kernelFunction(w, T):\n",
    "    y = np.outer(1/T/kB, w) # m x n matrix\n",
    "    return y*y*np.exp(-y)/(np.exp(-y)-1)**2\n",
    "\n",
    "w = makeW(n)\n",
    "T = makeT(m)\n",
    "kernel = backend.variable(np.array([kernelFunction(w,T) for i in range(32)]))\n",
    "\n",
    "def customLoss(y_true, y_pred):\n",
    "    cV_true = backend.batch_dot(kernel, y_true)\n",
    "    cV_pred = backend.batch_dot(kernel, y_pred)\n",
    "    a = mse(y_true, y_pred)\n",
    "    b = mse(cV_true, cV_pred)\n",
    "    cost = a + b\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = backend.placeholder((32,240))\n",
    "b = backend.placeholder((32,300,240))\n",
    "c = backend.batch_dot(b,a)\n",
    "mse(c,c)\n",
    "mse(a,a)+mse(c,c)\n",
    "customLoss(a,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seeds for repeatable behaviour\n",
    "np.random.seed(140590)\n",
    "set_random_seed(140590)\n",
    "\n",
    "# here we define the model architecture\n",
    "model = Sequential()\n",
    "\n",
    "#input layer\n",
    "model.add(Conv2D(2, (2, 2), input_shape=(2, m, 1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "print model.output_shape\n",
    "\n",
    "model.add(Flatten())\n",
    "print model.output_shape\n",
    "\n",
    "model.add(Dense(240,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "print model.output_shape\n",
    "\n",
    "model.add(Dense(n, activation='softmax'))\n",
    "print model.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we compile the model and define a loss function\n",
    "model.compile(loss='mean_squared_error', optimizer='Nadam', metrics=['mean_squared_error', r_square])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enabling early stopping based on mean_squared_error\n",
    "earlystopping = EarlyStopping(monitor='mean_squared_error', patience=10, verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can fit the model\n",
    "result = model.fit(X_train, Y_train, batch_size=32, epochs=240, verbose=1, validation_data=(X_test, Y_test), callbacks=[earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions\n",
    "Y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training curve for R^2 (beware of scale, starts very low negative)\n",
    "plt.plot(result.history['r_square'])\n",
    "plt.plot(result.history['val_r_square'])\n",
    "plt.title('model R^2')\n",
    "plt.ylabel('R^2')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "           \n",
    "# plot training curve for rmse\n",
    "plt.plot(result.history['mean_squared_error'])\n",
    "plt.plot(result.history['val_mean_squared_error'])\n",
    "plt.title('mean_squared_error')\n",
    "plt.ylabel('mean_squared_error')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print statistical figures of merit\n",
    "\n",
    "import sklearn.metrics, math\n",
    "print(\"\\n\")\n",
    "print(\"Mean absolute error (MAE):      %f\" % sklearn.metrics.mean_absolute_error(Y_test,Y_pred))\n",
    "print(\"Mean squared error (MSE):       %f\" % sklearn.metrics.mean_squared_error(Y_test,Y_pred))\n",
    "print(\"Root mean squared error (RMSE): %f\" % math.sqrt(sklearn.metrics.mean_squared_error(Y_test,Y_pred)))\n",
    "print(\"R square (R^2):                 %f\" % sklearn.metrics.r2_score(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(0,q)\n",
    "print i\n",
    "\n",
    "plt.plot(Y_pred[i])\n",
    "plt.plot(Y_test[i])\n",
    "\n",
    "plt.legend(['pred', 'real'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 7746\n",
    "print i\n",
    "\n",
    "plt.plot(Y_pred[i])\n",
    "plt.plot(Y_test[i])\n",
    "plt.legend(['pred', 'real'], loc='upper right')\n",
    "plt.axis([0,30,0,0.3])\n",
    "plt.show()\n",
    "\n",
    "i = 4160\n",
    "print i\n",
    "\n",
    "plt.plot(Y_pred[i])\n",
    "plt.plot(Y_test[i])\n",
    "plt.legend(['pred', 'real'], loc='upper right')\n",
    "plt.axis([0,100,0,0.06])\n",
    "plt.show()\n",
    "\n",
    "i = 7606\n",
    "print i\n",
    "\n",
    "plt.plot(Y_pred[i])\n",
    "plt.plot(Y_test[i])\n",
    "plt.legend(['pred', 'real'], loc='upper right')\n",
    "plt.axis([0,100,0,0.06])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model architecture, weights, training config, and state to HD5 file\n",
    "model.save('./hc-CNN_005_linear_NAdam_MSE_e064_b32.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model-CNN-005.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = np.linspace(1,300,300)\n",
    "w = makeW(240)\n",
    "i=50\n",
    "plt.plot(Y_pred[i])\n",
    "plt.plot(Y_test[i])\n",
    "plt.legend(['pred', 'real'], loc='upper right')\n",
    "plt.show()\n",
    "plt.plot(T,cv(Y_pred[i],T))\n",
    "dat = X_test[i].reshape((2,128))\n",
    "plt.plot(dat[0]*300,dat[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
